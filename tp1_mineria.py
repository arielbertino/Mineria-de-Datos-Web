# -*- coding: utf-8 -*-
"""TP1_Mineria.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wl6JCZqt42tZcthMPFhHot5xzhO0pjnc
"""

# Commented out IPython magic to ensure Python compatibility.
# INICIO TP1 MINERIA WEB

# IMPORTACION DE LIBRERIAS
!pip install contractions
!pip install inflect

import numpy
import requests
import contractions
import nltk
import unicodedata
import re
import inflect
import matplotlib.pyplot as plt

nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('gutenberg')

from bs4 import BeautifulSoup
from nltk import sent_tokenize
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
from nltk.book import *
# %matplotlib inline

nltk.download('punkt')      #instala contenedor de tokenizadores
nltk.download('stopwords')  #instala contenedores de stopwords
nltk.download('book')       #instala textos de prueba de nltk


# ELIMINACION DE RUIDO
## OBTENCION DE PAGINA - HTML
### SETEO DE HEADER PARA OBTENER PAGINA

cabecera = requests.utils.default_headers()
cabecera.update({ 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'})
direccion = "http://www.angelfire.com/hi/littleprince/framehome.html"
direccionBase = "http://www.angelfire.com/hi/littleprince/"
peticion = requests.get(direccion, cabecera)
mainPage = BeautifulSoup(peticion.content, 'html.parser')

""" el siguiente for busca todos los tags target="frame"
    en el html de la pagina inicial (en "direccion")
"""
contenidoBruto=""                                                         #acumulador de texto

for URL in mainPage.find_all(target="frame"): 
    dirCap = direccionBase + URL.get("href")                              #obtengo la URL del capitulo n
    p = requests.get(dirCap, cabecera)                                    #genero peticion de URL
    capitulo = BeautifulSoup(p.content, 'html.parser')                    #defino capitulo como BeautifulSoup de la peticion y un parser html
    parrafos = capitulo.find("p")                     #[consultar]        #encuentro el primer parrafo del capitulo, tuve problemas, si no usaba esto, me encontraba 12 veces cada parrafo, muy raro, porque podria llegar a pasa?(consultar)
    contenidoBruto = contenidoBruto + repr(parrafos.get_text())           #agrego a contenidoBruto el text del parrafo n con cada iteracion del fo 

contenidoBruto = contenidoBruto.replace("\\r", "")                           #borrar espacios en blaco
contenidoBruto = contenidoBruto.replace("\\n", "")                           #borrar saltos de linea
contenidoBruto = contenidoBruto.replace("\\",  "")  #[consultar todo esto]   #borrar barra suelta que hay por el texto
contenidoBruto = contenidoBruto.replace(".",". ")                            #por algun motivo me quedaban palabras unidas con puntos, momentaneamente asi lo solucione
contenidoBruto = contenidoBruto[1:]                                          #quedaba una comilla al prinicipio, con esta sentencia la borré

"""
    Tuve problemas para ir generando el texto
    de los parrafos sin los espacios (\r) ni saltos
    de lineas (\n) usando stripped_strins,
    con lo cual opte por usar replace("stringAReemplazar","nuevoString")
    de las varibles string para poder continuar
"""
contenidoExpandido = contractions.fix(contenidoBruto)

# TOKENIZACION

## SEPARACION EN ORACIONES O SEGMENTAR (PASO OPTATIVO)
 
#oraciones = sent_tokenize(contenidoExpandido)         #segmentando en oraciones, al tokenizar en palabras debia operar como listas que luego arrojaba error al elinar ascii

# SEPARACION EN PALABRAS

palabras = word_tokenize(contenidoExpandido)                #le agrego a la lista de palabras la tokenizacion de cada oracion

# NORMALIZACION
## ELIMINACION DE CARACTERES NO REPRESENTABLES EN ASCII

def eliminarNoAscii(listaPalabras):
    arrPalabras = []
    for pal in listaPalabras:
        nuevaPal = unicodedata.normalize('NFKD', pal).encode('ascii', 'ignore').decode('ascii', 'ignore')  
        arrPalabras.append(nuevaPal)
    return arrPalabras

## CONVERTIR PARALABRAS A MINUSCULAS

def convertirAMin(listaPalabras):
    arrPalabras = []
    for pal in listaPalabras:
        nuevaPal = pal.lower()
        arrPalabras.append(nuevaPal)
    return arrPalabras

## ELIMINAR SIGNOS DE PUNTUACION

def eliminarPuntuacion(listaPalabras):
    arrPalabras = []
    for pal in listaPalabras:
        nuevaPal = re.sub(r'[^\w\s]', '', pal)  
        if nuevaPal != '':
            arrPalabras.append(nuevaPal)
    return arrPalabras

## INTERCAMBIAR NUMERO-PALABRA

def cambioNumPal(listaPalabras):
    motorDeCambio = inflect.engine()
    arrPalabras = []
    for pal in listaPalabras:
        if pal.isdigit():
            print("Encontré un dígito ",pal)
            nuevaPal = motorDeCambio.number_to_words(pal)
            arrPalabras.append(nuevaPal)
        else:
            arrPalabras.append(pal)
    return arrPalabras

# ELIMINACION DE STOPWORDS

def borrarStopwords(listaPalabras):
    arrPalabras = []
    for pal in listaPalabras:
        if pal not in stopwords.words('english'):
            arrPalabras.append(pal)
    return arrPalabras

# REDUCCION MEDIANTE STEMMING
## STEMMER DE PORTER

stemmerPorter = nltk.PorterStemmer()

## STEMMER DE LANKASTER

stemmerLankaster = nltk.LancasterStemmer()

print("Lista de palabras en bruto: ")
print(palabras)                                    #por algun motivo estas dos listas son iguales
print()

palabras2 = eliminarNoAscii(palabras)
print("Lista de palabras sin ascii no representable: ")
print(palabras2)                                   #por algun motivo estas dos listas son iguales
print()

palabritas= convertirAMin(palabras2)
print("Lista de palabras en minúscula: ")
print(palabritas)                                  #mostrar tokens en minuscula
print()

soloPalabras = eliminarPuntuacion(palabritas)
print("Lista de palabras sin signos de puntuacion: ")
print(soloPalabras)                                #mostrar lista sin signos de puntuacion
print()

palabrasSinNumeros = cambioNumPal(soloPalabras)
print("Lista de palabras sin números: ")
print(palabrasSinNumeros)                          #mostrar palabras sin numeros, pero por algun motivo a veces deja numeros en tokens distintos y aveces es como que corta en token partes de un numero gran(largo en palabras) [consultar]  
print()

#en este punto trate de volver a tokenizar y eliminar signis de puntuacion de nuevos pero sin resultados [consultar]
#continue con el proceso a pesar de tenek tokens errados

palsSinStopwords = borrarStopwords(palabrasSinNumeros)
print("Lista de palabras sin stopwords: ")
print(palsSinStopwords)
print()

# OBTENER LISTA DE STOPWORDS

listaDeStopwords = []
for tok in palabrasSinNumeros:
    if tok not in palsSinStopwords:
        listaDeStopwords.append(tok)

print("Lista de stopwords: ")
print("Estas stopwords fueron descartadas yas que al repetirse constantemente, ")
print("no ofrecen informacion sustancial hacerca del contenido del texto.")
print(listaDeStopwords)
print()

for pal in stopwords.words('english'):
  print(pal)
""" 
    a continuacion itenté utilizar uno los stemmers que vimos pero con 
    el porter cortaba varias palabras pero con el lankaster era muy radical,
    cortaba las palabras a su minima expresion, si es que asi se lo puede 
    llamar, con lo cual opté por no utilizar algoritmos de stemming          
"""

#list(stemmerPorter.stem(token) for token in palsSinStopwords)       #prueba con stemmer porter
#list(stemmerLankaster.stem(token) for token in palsSinStopwords)    #prueba con stemmer lankaster

textoParaPlot = nltk.text.Text(palsSinStopwords)    #tuve que hacer un casting porque no encontraba la forma de poder hacer el plot de dispersion de palabras
textoParaPlot.dispersion_plot(["little", "prince", "sheep", "planet", "astronomer", "king", "rose"])

#CONCLUSIONES DEL PLOT DE DISPERSIONES
print()
print("Conclusiones del grafico:")
print()
print("La palabra \"little\" se repite constantemente a lo largo de todo el texto")
print("La palabra \"prince\" se repite constantemente a lo largo de todo el texto pero en menor medida que \"little\" ")
print("La palabra \"sheep\" se repite de manera concentrada al inicio y muy poco al final")
print("La palabra \"planet\" se repite mas o menos uniformemente sobre los primeros tres cuartos del texto")
print("La palabra \"astronomer\" se concentra en una parte muy pequeña del texto")
print("La palabra \"king\" bastante sobre la parte media del texto")
print("La palabra \"rose\" se repite poco en algunas partes del ultimo tercio del texto")

