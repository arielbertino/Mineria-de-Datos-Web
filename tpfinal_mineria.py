# -*- coding: utf-8 -*-
"""TPFINAL_Mineria.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1saNPFxSzkgL5vzIZD5IFEXd1U144VE2A

## MINERÍA DE DATOS WEB - TRABAJO FINAL


Utilizando el dataset del Trabajo Práctico Nro 3 (Review Polarity) desarrollar los siguientes puntos:

>1. Realizar el pre-procesamiento de los textos aplicando al menos dos técnicas de las estudiadas. Analice los cambios en el dataset en cada paso (por ejemplo reducción de dimensionalidad).

>2. Utilizando la técnica de Hold-out, divida el dataset en un conjunto para entrenamiento (80%) y otro para prueba (20%)

>3. Con el dataset de entrenamiento, evaluar al menos 3 clasificadores de los vistos en clase para la clasificación de reviews en positivos/negativos. Seleccionar los hiperparámetros que considere adecuados en cada clasificador. Utilizando k-fold cross validation, compare y elabore conclusiones a partir de los resultados de los tres clasificadores, considerando no menos de 3 métricas.

>4. Investigue y explique en qué consiste la técnica conocida como RandomForest y cuáles son sus principales características.

>5. Entrenar un clasificador RandomForest a partir de los datos del dataset de
entrenamiento obtenido en el punto 2 y compare sus resultados con los obtenidos por los clasificadores del punto 3 utilizando k-fold cross validation.

>6. Evalué los cuatro clasificadores entrenados con el conjunto de datos de prueba reservado en el punto 2. Elabore conclusiones sobre el desempeño de los
clasificadores.

**ENTREGAR UN INFORME DETALLANDO LOS PASOS ANTERIORES Y EL/LOS NOTEBOOKS CORRESPONDIENTES TENER EN CUENTA PARA DESARROLLAR EL TRABAJO LOS COMENTARIOS RECIBIDOS EN LA DEVOLUCIÓN DEL TP 3**

## **FECHA DE ENTREGA ​15 DE SEPTIEMBRE**
"""

# IMPORTACION DE LIBRERIAS

!pip install inflect                                         # Instalar librería para transformar números a palabras
!pip install chardet                                         # Instalar librería para detectar la codificación de los tipo byte ('ascii', 'utf-8', etc)
!pip install sklearn                                         # Instalar librería skitlear contenedor de muchas herrameintas necesarias 
!pip install contractions                                    # Instalar librer;ia para quitar contracciones del texto
!pip install scikit-learn                                    # Instalas ScikitLearn en entorno local
!pip install nltk                                            # Instalas Natural Language Tool Kit en entorno local

from sklearn import datasets                                 # Para operar con datasets
from pprint  import pprint                                   # Para imprimir mejor las cosas
from nltk import word_tokenize                               # Para tokenizar a nivel palabras  
from sklearn.feature_extraction.text import TfidfVectorizer  # Para usar un vectorizador TF-IDF
from sklearn.feature_extraction.text import CountVectorizer  # Para usar Vectorizador
from sklearn.feature_extraction.text import TfidfTransformer # Para usar un transformador TF-IDF
from sklearn.model_selection import train_test_split         # Para generar conjuntos de entrenamiento y prueba
from sklearn.svm import SVC                                  # Para utilizar Maquina de vectores de soporte
from sklearn.naive_bayes import MultinomialNB                # Para utilizar Naive-Bayes Multinomial
from sklearn import neighbors                                # Para Utilizar KNN Vecinos Más Cercanos
from sklearn.ensemble import RandomForestClassifier          # Para usar Random Forest
from sklearn.pipeline import Pipeline                        # Para usar mecanismos de Pipeline
from sklearn.model_selection import GridSearchCV             # Para utilizar GridSearch en busqueda exexhaustiva de parámetros
from sklearn.metrics import confusion_matrix                 # Para poder visualizar la Matriz de Confusión
from sklearn.metrics import classification_report            # Para generar un reporte de las métricas a comparar
from sklearn.model_selection import KFold                    # Para usar validacion cruzada estándar
from sklearn.model_selection import StratifiedKFold          # Para usar validacion cruzada estratificada
from sklearn.model_selection import ShuffleSplit             # Para usar validacion cruzada aleatoria
from sklearn.model_selection import StratifiedShuffleSplit   # Para usar validacion cruzada aleatoria estratificada
from sklearn.model_selection import cross_val_score          # Para generar vector de puntajes de un clasificador dado
from sklearn.model_selection import cross_validate           # Para evaluar varias métricas a la vez
from nltk.corpus import stopwords                            # Para usar una lista predefinida de stopwrods en inglés
    
import nltk                                                  # Para procesamiento de lenguaje natural
import re                                                    # Para eliminar signos de puntuación, expresiones regulares, entre otras
import contractions                                          # Para expandir constracciones de palabras
import unicodedata                                           # Para eliminar caracteres unicode
import chardet                                               # Para detectar la codificacion de un tipo byte
import numpy as np                                           # Para realizar operaciones varias
import time                                                  # Para usar funcines de medición de tiempo
import inflect                                               # Para realizar tranformaciones de texto
nltk.download('punkt')                                       # Descargar contenedor de tokenizadores
nltk.download('stopwords')                                   # Descargar contenedores de stopwords

# Defino algunos colores para imprimir mejor las cosas
negro    = '\u001b[30m'
rojo     = '\u001b[31m'
verde    = '\u001b[32m'
amarillo = '\u001b[33m'
azul     = '\u001b[34m'
magenta  = '\u001b[35m'
cyan     = '\u001b[36m'
blanco   = '\u001b[37m'
reset    = '\u001b[0m'

# Comando para ejecutar entorno local
#jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=9090 --no-browser

# Rutas de acceso al dataset solicitado (Review Polarity)
# Carpeta publica de drive con las reseñas 
rutaExterna = "https://drive.google.com/drive/folders/198RLXEf2wQRXsE0P76lnzTzUrolmCqJI?usp=sharing" #por si la quieren acceder desde afuera
# Ruta para trabajar desde colab
ruta = "/content/drive/My Drive/Colab Notebooks/txt_sentoken"                                        
# Ruta local desde la PC
rutaLocal = "/Users/Ariel/Desktop/txt_sentoken"

"""## **1.**
Se realiza un tratamiento previa al pre-procesamiento solicitado en el inciso ya que para aplicar pre-procesamiento a las reseñas estas deben ser tipo string(str) y al cargar el dataset en la variable "datos" las reseñas se cargan con tipo byte codificado en 'ascii'. Se detalla este paso previo más abajo.

> Cargo el dataset a la variable datos: Esta variable es un contendor al cual yo lo puedo pedir diferentes cosas:

* datos.data: retorna la lista de todas la reseñas  
* datos.target_names: retorna lista con nombres de todas las clases
* datos.target: retorna lista indicando a que clase pertenece la i-ésima reseña
* datos.DESCR: muestra un descripcion del dataset si la tuviera
* datos.filenames: muestra el nombre de los archivos de donde provienen las reseñas
"""

datos = datasets.load_files(rutaLocal)
cantRese = len(datos.data)

# Pruebas que fui haciendo

#print(type(datos))                                       # Retorna "sklearn.utils.Bunch"
#print(type(datos.data[1]))                               # Retorna que la primer reseña es tipo byte
#print(chardet.detect(datos.data[1])['encoding'])         # Detecto la codificacion de la reseña, retorna que esta codificada como 'ascii'
#datos.data[1]=bytes(reseniaExpandida,encoding="ascii")   # Codifico una reseña con formato 'ascii'

# TRATAMIENTO PREVIO AL PRE-PROCESAMIENTO
# Defición de funciones varias

# Fución que castea una lista a string
def castearAString(lista):
  for d in range(cantRese):
    lista[d] = str(lista[d])
  return lista

# Fución que castea una lista a bytes codificado en ascii
def castearAByteAscii(lista):
  l=lista
  for d in range(cantRese):
    st = l[d]
    l[d] =  l[d].encode('ascii')
  return l

# Función que usaré para ir mostrarndo los resultados intermedios sin tener 
#  que mostrar las 2000 reseñas juntas. Considerar que al usar pprint 
#  formatea el texto para que puedan ser visibles las secuencias de escape
#  que normalmente no tienen que ser mostradas
def imprimirReseniaN(res, n):
  if (n in range(cantRese)):
    print(azul)
    pprint(res[n])
    print(reset)
  else:
    print(rojo+"Reseña solicitada fuera de rango"+reset)

# La siguiente función elimina ciertos caracteres de las reseñas
def limpiezaSuperficialParcial(resenia):  
  r = resenia.replace("\\n","")       # Elimina saltos de linea
  r = r.replace("\\b","")             # Elimina algun que otro backspace
  r = r.replace(" . ",". ")           # Acomoda espacios antes de puntos
  r = r.replace(" , ",", ")           # Acomoda espacios antes de comas
  r = r.replace(" ? ","? ")           # Acomoda espacios antes de signos de preguntas
  r = r.replace("\\","")              # Elimina bastantes dobles barras que hay en todas las reseñas
  return r

# Función que dada una lista de reseñas, a cada una le aplica un 
# tratamiento previo al preprocesamiento
def limpiezaSuperficialTotal(lista):
  l = lista
  for i in range(cantRese):
    l[i] = limpiezaSuperficialParcial(l[i])
  return l

# Función que se encarga de expandir las contracciones propias del ingles
# Aclaración: Por algún motivo no expande todas las contracciones
#             Después de revisar algunas reseñas, quizás el motivo
#             sea porque antes de la comilla de la contracción (ej: 's; 'd)
#             hay secuencias(o caracteres) de escape que hacen que la librería
#             no detecte la contraccón, con lo cual la deja igual a como ya estaba
!pip install contractions
def expandirContraccion(resenia):
  r = contractions.fix(resenia)
  return r

# Función que a cada reseña le aplica la función anterior expande contracciones
def expandirTodasContracciones(lista):
  l = lista
  for i in range(cantRese):
    l[i] = expandirContraccion(l[i])
  return l

"""En la celda de código siguiente se realiza el tratamiento previo al preprocesamiento"""

resSinPreProcByte = datos.data                                  # Extraigo la lista de las reseñas en byte
#imprimirReseniaN(resSinPreProcByte, 15)                        # Mostrar para ver paso intermedio

resSinPreProcStr  = castearAString(resSinPreProcByte)           # Casteo las reseñas a string
reseniasLimpias = limpiezaSuperficialTotal(resSinPreProcStr)    # Tratamiento previo
#imprimirReseniaN(reseniasLimpias, 15)                          # Mostrar para ver paso intermedio

"""A continuación procedo a realizar uno de los preprocesamientos solicitados, el que utilicé fue:

* Eliminacion de contracciones

> ACLARACIÓN IMPORTANTE: En esta fase del análisis no combiene realizar más preprocesamientos, ya que se pueden aplicar el etapa de busqueda exhaústiva de parámetro con gridSearch. Con lo cuál voy a aplicar eliminación de stopwords y tokenizar y otros preprocesamientos más adelante.

"""

#imprimirReseniaN(reseniasLimpias, 10)   # Mostrar para ver paso intermed

# Aplico expansión de contracciones
reseniasExpandidas = expandirTodasContracciones(reseniasLimpias)
#imprimirReseniaN(reseniasExpandidas, 15)                        # Mostrar para ver paso intermedio

"""Por último como paso previo al la técnica de hold-out, tengo que dejar el data set en el mismo tipo con el cuál lo cargué(bytes codificado ascii), entonces realizo lo siguiente.

Aclaracion: la linea 1 de la siguiente celda, no ejecutarla dos veces seguidas, se genera un error  "'bytes' object has no attribute 'encode'". Precaución al abrir el notebook de colab ya que aveces se ejecutan todas las celdas automáticamente.
"""

reseniasConPreProc = castearAByteAscii(reseniasExpandidas)

# Asigno nuevamenta la lista de reseñas generadas hasta el 
#  momento a datos.data (originalmete las reseñas)
datos.data = reseniasConPreProc
#imprimirReseniaN(datos.data,1999)     # Mostrar para ver paso intermedio

"""## **2.**

Se realiza hold-out dividiendo el conjunto de datos en 80% para el conjuntos de entrenamiento(train) y 20% para el conjunto de prueba(test). El parámetro 'Shuffle=True', permite mezclar el dataset antes de realizar efectivamente la partición en ambos conjuntos. Para todo el desarrollo del trabajo práctico se dejo con su valor por defecto ('shuffle=False')
"""

datosEntrenamiento, datosPrueba, clasesRealesDEntre,clasesRealesDPrueba = train_test_split(datos.data, datos.target, test_size=0.20 """shuffle=True""")

"""## **3.**

Los clasificadores que voy a utlizar son Máquina de vectores de soporte, naive-bayes multinomial y vecinos más cercanos.

Con el objetivo de seleccionar correctamente los hiperparámetros usaré gridSearch mediante un mecanismo de pipelines para poder operar de manera más ordenada.

> Defino un vectorizador

> Defino un transformador TF-IDF

> Defino los tres clasificadores descriptos más arriba

Defino varios pipelines que usaré en la función "oraculoDeDelfos" para predecir las clases de las reseñas (documentos-ejemplos). 

Comenté el pipeline con el clasificados Naive-bayes gaussiano ya que como trabaja con datos continuos pedia convertir los detos a una matriz densa. Decidí omitir su uso ya que los datos que se usan en este trabajo con discretos, así como también el uso de un método de regresión lineal, ya que arrojaba resultados demasiado dispares comparado con otros métodos.

En la siguiente celda de código se realizan pruebas con los clasificadores, "por defecto", no se le modificó ningun hiperparámetro, esta tarea de realizará más adelante, utilizando gridSearch (busqueda exahustiva de hiperparámetros).
"""

# Definición de clasificadores, verbose=True hace que se muestre el tiempo invertido en cada paso del pipeline dado.
# A la hora de probar este código, si se desea, descomentar el parametro verbose=True asi en la sadia se podrá 
# apreciar el tiempo de entrenamiento de cada paso del pipeline, lo comenté ya que a la hora de hacer gridSearch
# me mostraba muchos datos debido a la cantidad de combinaciones que que el algorto va probando.

clasificadorMVS = Pipeline([ ('VECTORIZADOR', CountVectorizer()), ('TRANSFORMADOR-TFIDF', TfidfTransformer()), ('CLASIFICADOR', SVC())], verbose=True)
clasificadorNBM = Pipeline([ ('VECTORIZADOR', CountVectorizer()), ('TRANSFORMADOR-TFIDF', TfidfTransformer()), ('CLASIFICADOR', MultinomialNB())], verbose=True)
clasificadorKNN = Pipeline([ ('VECTORIZADOR', CountVectorizer()), ('TRANSFORMADOR-TFIDF', TfidfTransformer()), ('CLASIFICADOR', neighbors.KNeighborsClassifier())], verbose=True)

def oraculoDeDelfos(clasi, dat):
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)
  print(azul+"Pipeline usado: ",clasi.named_steps,""+reset)                                       #named_steps muestra los pasos que tiene el pipeline
  print(amarillo+"Tiempo invertido en cada paso del pipeline, en fase de entrenamiento:"+reset)
  tiempoInicio = time.time()
  p = clasi.fit(dat.data, dat.target)
  predecido = p.predict(dat.data)
  tiempoTotal = time.time() - tiempoInicio
  print(verde+"Tiempo Total invertido en entrenar y predecir:", "{0:.4f}".format(tiempoTotal),"segundos"+reset)
  mediaDeAcierto = "{0:.3f}".format(np.mean(predecido == dat.target) * 100)                #format me permite mostrar una cantidad de decimales dada
  print(rojo+"Clases obtenidas:",predecido, ", Porcentaje de exito:", mediaDeAcierto, "%"+reset)
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)

# Las siguientes lineas tardan en ejecutarse entre 30 y 45 segundos 
# en un procesador intel i7 de 2.6 GHz con 4 Nucles y 16GB de ram,
# al utilizar todos los clasificadores seguidos, para que no insuma
# tanto tiempo de corrido separé las tres invocaciones a la función
# en tres celdas de código distintas

"""**Aclaración**: lo realizado son pruebas que llevan a encontrar los mejores hiperparámetros para los clasificadores utilizados llegado el momento se especificará la combinacion encontrada entre otros datos de interés. Acontinuación se ejecutan tres clasificadores de manera estándar para ver resultados preliminares utlizando todo el set de datos tanto para entrenamiento como para prueba."""

# Máquina de vectores de soporte
oraculoDeDelfos(clasificadorMVS, datos)

# Naive-Bayes Multinomial
oraculoDeDelfos(clasificadorNBM, datos)

# Vecinos Más Cercanos
oraculoDeDelfos(clasificadorKNN, datos)

"""Se puede observar en los resultados anteriores que, en orden de mayor a menor, la tasa de acierto entre las clases reales y las clases predecidas son: 

|Clasificador|Porcentaje acierto|  
|--------------------------|----|
|SVM                       |99.8|
|NB Multinomial            |95.7|
|KNN                       |68.9|

Se puede observar como el tiempo para vectorizar y para TF-IDF es aproximandamete el mismo en todos los pipelines (ya que son los mismo para todos), pero cambia sustancialemte en tiempo de la predicción con la máquina de vectores de soporte ya que tiene que recorrer todos los documentos previos para clasificar un documento nuevo.

También cabe aclarar que tanto los conjuntos de entrenamiento y prueba son los mismos (el dataset completo), con lo cual se esperaría que el procentaje de acierto en las prediciones fuese alto (esto se ve con SVM y Naive-Bayes Multinomial). Esto se debe a que el modelo se sobreentrenó(overfiting), con lo cuál el modelo esta muy centrado en los datos de entrenamiento y el ser iguales a los de prueba arroja una taza de acierto alta. A la hora de clasificar nuevos documentos posiblemente arroje errores donde no lo sea.

Pero es relativamente bajo el porcenteje de error al usar KNN(vecinos más cercanos). Esto se debe a la naturaleza intrínseca de propio algorítmo, donde cada documento es un punto en el espacio y se comparan los documentos en el espacio con el documento a ser clasificado. Esto generará un bajo nivel de acierto ya que la clasificación se basa en los vecino más cercanos que tenga ese nuevo documento.

---

Continuando en la busqueda de la mejor combinación de hiperparámetros se usará la siguiente función que de la mano con la técnica GridSearch se encarga de buscar/testear de manera exhaustiva todas las combinaciones(definidas arbitratiramente) de parámetros con el fin de reducir el tiempo de clasificación. También se define una función en la cual se establece un preprocesamiento personalizado que se le aplicará en la parte encargada de vectorizar los documentos, en la celda de código se indicará precisamente cuál. Decidí no utilizar ningun método de stemming ya que al llevar la palabras a su raíz quizas no sea lo ideal en este tipo de documentos, ya que cada reseña es particular de cada persona con lo cuál quizás dos reseñas distintas posean la misma palabra pero con significados distintos y el stemmer los reduciría a la misma raíz cuando no necesariamente sea lo correcto.
"""

def encontrarLoMejor(pipeline, parametros, datosTrain, clasesTrain): 
  print(negro+"---------------------------------------------------------------------------------------------------------------------------------------------------------"+reset)
  print(amarillo+"Pipeline usado: ",pipeline.named_steps,""+reset)
  tiempoInicio = time.time()
  g = GridSearchCV(pipeline, parametros, cv=5, n_jobs=-1)
  g = g.fit(datosTrain, clasesTrain)
  tiempoTotal = time.time() - tiempoInicio
  print(verde+"Tiempo Total invertido en encontrar la mejor combinación de hiperparámetros:", "{0:.4f}".format(tiempoTotal),"segundos"+reset)
  print(azul+"La mejor taza de acierto fue de:", "{0:.4f}".format(g.best_score_ * 100),""+reset)
  print(rojo+"Con los siguientes valores de parámetros:"+reset)
  print(rojo+"",g.best_params_,reset)
  print(negro+"---------------------------------------------------------------------------------------------------------------------------------------------------------"+reset)

def prePrcesamientoPersonalizado(texto):
  t = texto.lower()                                                                         # Texto a minúscula
  t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii', 'ignore')  # Eliminación de caracteres no representables en ascii
  t = re.sub(r'[^\w\s]',"", t)                                                              # Eliminación de signos de puntuación
  t = re.sub("\\W"," ", t)                                                                  # Eliminar caracteres especiales
  # Intenté reemplazar números por palabras, con las siguientes dos líneas de código pero en lugar de reemplazar número a número lo que hacía es eliminar
  # todo el texto y todos los números que encuentra los toma como uno solo, asi que lo dejé comentado, ya que se perdía la totalidad de la reseña
  #motorDeCambio = inflect.engine()                                       
  #t = motorDeCambio.number_to_words(t)                                                         
  return t

# Defino la lista de parametro para cada clasificador

parametrosMVS = {'VECTORIZADOR__ngram_range':[(1,1),(1,2)], 'VECTORIZADOR__stop_words': ['english'], 'VECTORIZADOR__tokenizer':[word_tokenize], 
                 'VECTORIZADOR__preprocessor':[prePrcesamientoPersonalizado], 'TRANSFORMADOR-TFIDF__use_idf': [True,False], 
                 'CLASIFICADOR__kernel': ['linear','poly','rfb','sigmoid'], 'CLASIFICADOR__coef0': [1e-2, 1e-3], 'CLASIFICADOR__C':[1.0, 2.0] }    
parametrosNBM = {'VECTORIZADOR__ngram_range':[(1,1),(1,2),(1,3)], 'VECTORIZADOR__stop_words': ['english'], 'VECTORIZADOR__tokenizer':[word_tokenize], 
                 'VECTORIZADOR__preprocessor':[prePrcesamientoPersonalizado], 'TRANSFORMADOR-TFIDF__use_idf': [True,False], 'CLASIFICADOR__alpha': [1e-2, 1e-3, 1e-4] }
parametrosKNN = {'VECTORIZADOR__ngram_range':[(1,1),(1,2)], 'VECTORIZADOR__stop_words': ['english'], 'VECTORIZADOR__tokenizer':[word_tokenize], 
                 'VECTORIZADOR__preprocessor':[prePrcesamientoPersonalizado], 'TRANSFORMADOR-TFIDF__use_idf': [True,False], 
                 'CLASIFICADOR__leaf_size': [15, 30, 45], 'CLASIFICADOR__n_neighbors': [2, 5, 8] }

# las siguientes lineas tardan en ejecutarse entre 17 y 20 minutos 
# en un procesador intel i7 de 2.6 GHz con 4 Nucles y 16GB de ram,
# al utilizar todas las invocaciones seguidas, para que no insuma
# tanto tiempo de corrido separé las tres invocaciones a la función
# en treso celdas de código distintas

"""Nota: Para todos los clasificadores a la hora de la vectorización de los documentos, los preprocesamientos que les otrogué fueron: 

* Eliminación de stop-word usando la lista de stop-words 'english'
* Como tokenizador, el 'word_tokenize' de la librería nltk
* La función 'prePrcesamientoPersonalizado' definida por mí
"""

encontrarLoMejor(clasificadorMVS, parametrosMVS, datosEntrenamiento, clasesRealesDEntre)

encontrarLoMejor(clasificadorNBM, parametrosNBM, datosEntrenamiento, clasesRealesDEntre)

encontrarLoMejor(clasificadorKNN, parametrosKNN, datosEntrenamiento, clasesRealesDEntre)

"""Antes de presentar la información de las mediciones, se debe aclarar que se parcionó el dataset original en particiones 80/20 para los conjuntos de entrenamiento y de prueba respectivamente. Tambien se volvían a calcular entre prueba y prueba para para tener un panorama más amplio de a situacion

>Mediante la aplicación de los métodos anteriores se obtuvo: 

> Al usar el clasificador SVC (maquina de vectors de soporte), con una taza de acierto de aproximadamente 83.8750  %, los siguientes valores:

**Pipeline usado**: *{'VECTORIZADOR': CountVectorizer(), 'TRANSFORMADOR-TFIDF': TfidfTransformer(), 'CLASIFICADOR': SVC()}*

**Tiempo Total invertido en encontrar la mejor combinación de hiperparámetros**: 675.7140 segundos

**Con los siguientes valores de parámetros**:  *{'CLASIFICADOR__C': 2.0, 'CLASIFICADOR__coef0': 0.01, 'CLASIFICADOR__kernel': 'linear', 'TRANSFORMADOR-TFIDF__use_idf': True, 'VECTORIZADOR__ngram_range': (1, 1), 'VECTORIZADOR__preprocessor': <function prePrcesamientoPersonalizado at 0x000002A65ED37310>, 'VECTORIZADOR__stop_words': 'english', 'VECTORIZADOR__tokenizer': <function word_tokenize at 0x000002A65E25B820>}*

> Al usar el clasificador Naive-Bayes Multinomial, con una taza de acierto aproximada de 80 %, los siguientes valores:

**Pipeline usado**: *{'VECTORIZADOR': CountVectorizer(), 'TRANSFORMADOR-TFIDF': TfidfTransformer(), 'CLASIFICADOR': MultinomialNB()}*

**Tiempo Total invertido en encontrar la mejor conbinación de hiperparámetros**: 138.2859 segundos

**Con los siguientes valeres de parámetros**: *{'CLASIFICADOR__alpha': 0.01, 'TRANSFORMADOR-TFIDF__use_idf': False, 'VECTORIZADOR__ngram_range': (1, 3), 'VECTORIZADOR__preprocessor': <function prePrcesamientoPersonalizado at 0x000002A65ED37310>, 'VECTORIZADOR__stop_words': 'english', 'VECTORIZADOR__tokenizer': <function word_tokenize at 0x000002A65E25B820>}*

> Al usar el clasificador basado en Vecinos Más Cercanos, con una taza de acierto aproximada de 67.8750 %, los siguientes valores:
 
**Pipeline usado**:  *{'VECTORIZADOR': CountVectorizer(), 'TRANSFORMADOR-TFIDF': TfidfTransformer(), 'CLASIFICADOR': KNeighborsClassifier()}* 

**Tiempo Total invertido en encontrar la mejor combinación de hiperparámetros**: 228.5392 segundos

**La mejor taza de acierto fue de**: 67.8750 

**Con los siguientes valeres de parámetros**: *{'CLASIFICADOR__leaf_size': 15, 'CLASIFICADOR__n_neighbors': 8, 'TRANSFORMADOR-TFIDF__use_idf': True, 'VECTORIZADOR__ngram_range': (1, 1), 'VECTORIZADOR__preprocessor': <function prePrcesamientoPersonalizado at 0x000002A65ED36F70>, 'VECTORIZADOR__stop_words': 'english', 'VECTORIZADOR__tokenizer': <function word_tokenize at 0x000002A65E25B820>}*

>> Los valores de hiperparámetros hallados serán utilizados a la hora de comparar diversas métricas más adelante

---

Para continuar con el análisis de los datos procederemos a realizar y/o calcular:

> **Matriz de confusión**: Matriz que dispone los verdaderos positivos y verdaderos negativos(aciertos) en su diagonal y tambien el resto po por ejemplo los falsos negativos y los falsos positivos. con esta información se puede calcular distintas métricas con el fin de analisar que tan bine funciona un clasificador

> **Holdout**: Es un técnica para dividir los datos iniciales(o no), en un 
conjunto de entrenamiento y otro de prueba

> **Precisión**:  Indica, de las veces que se predijo de una clase, cuantas fueron correctas. De las clases que predije, que procentajo es de esa clase realmente

> **Recall:** intenta medir si yo puede identificar todos los documento pertecientes a una clase. Que porcentaje de los que yo cosideré aciertos soy capaz de reconocer

> **F-measure o F1**: Es una media armónica entre preción y recall. Indica una medida de calidad más general del modelo(clasificodor) utilizado.

> **F1-Macro**: F1 pero calculada para cada clase y retorna el promedio sin considerar el peso que representa dicha clase en el total de los documentos o dataset.

> **F-Weighted(ponderado)**: F1 pero calculada para cada clase y retorna el promedio considerando el peso que representa dicha clase en el total de los documentos o dataset ponderado por el soporte de casa clase. 

Debemos definir una función que muestre todas las métricas y también armar los clasificadores con los parámetros previamente encontrados. Si bine definí F1-macro y F1-Weighted(ponderado), son métricas que se evaluarán más adelante
"""

# Defino una función con la cual construiré la matriz de confusión, mostraré diferentes
# métricas

def oraculoDeDelfosMejorado(clasificador, datosEntrenamiento, datosPrueba, clasesRealesDEntre,clasesRealesDPrueba):
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)
  print(azul+"Pipeline usado: ",clasificador.named_steps,""+reset)                                                 # named_steps muestra los pasos que tiene el pipeline
  print(amarillo+"Tiempo invertido en cada paso del pipeline, en fase de entrenamiento:"+reset)       
  tiempoInicio = time.time()
  p = clasificador.fit(datosEntrenamiento, clasesRealesDEntre)                                                     # Entrenamiento apartir de datos y clases de entrenamiento
  clasesPredichas = p.predict(datosPrueba)                                                                         # Predicción apartir de datos de prueba
  tiempoTotal = time.time() - tiempoInicio                                                                         # medir el tiempo
  print(verde+"Tiempo Total invertido en entrenar y predecir:", "{0:.4f}".format(tiempoTotal),"segundos"+reset)
  mediaDeAcierto = "{0:.4f}".format(np.mean(clasesPredichas == clasesRealesDPrueba) * 100)                         # format me permite mostrar una cantidad de decimales dada
  print(rojo+"Porcentaje de exito obtenido de las clases predichas en relación a las clases reales de prueba:", mediaDeAcierto, "%"+reset)                  
  matrizConfusion = confusion_matrix(clasesRealesDPrueba, clasesPredichas)                                         # Genero la matriz de confusión
  print(magenta+"Matriz de confusión calculada apartir de"+reset)
  print(magenta+"las clases reales del conjunto de prueba y"+reset)
  print(magenta+"las clases predichas del conjunto de prueba",reset)
  print(magenta,matrizConfusion,reset)
  clases = ['Negativas', 'Positivas']
  print(cyan,classification_report(clasesRealesDPrueba, clasesPredichas, target_names=clases),reset)                # Genero un reporte con mostrando varias métricas 
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)
  
# Defino los clasificadores con los parámetros ya establecidos

clasificadorMVSTuneado = Pipeline([ ('VECTORIZADOR', CountVectorizer(ngram_range=(1,1), stop_words='english', tokenizer=word_tokenize, 
                                    preprocessor= prePrcesamientoPersonalizado)), ('TRANSFORMADOR-TFIDF', TfidfTransformer(use_idf=True)), 
                                    ('CLASIFICADOR', SVC(kernel='linear', coef0=1e-2, C= 2.0)) ], verbose=False )
clasificadorNBMTuneado = Pipeline([ ('VECTORIZADOR', CountVectorizer(ngram_range=(1,3), stop_words='english', tokenizer=word_tokenize, 
                                    preprocessor=prePrcesamientoPersonalizado)), ('TRANSFORMADOR-TFIDF', TfidfTransformer(use_idf=False)), 
                                    ('CLASIFICADOR', MultinomialNB(alpha=1e-2))], verbose=False )
clasificadorKNNTuneado = Pipeline([ ('VECTORIZADOR', CountVectorizer(ngram_range=(1,1), stop_words='english', tokenizer=word_tokenize, 
                                    preprocessor=prePrcesamientoPersonalizado)), ('TRANSFORMADOR-TFIDF', TfidfTransformer(use_idf=True)), 
                                    ('CLASIFICADOR', neighbors.KNeighborsClassifier(leaf_size=15, n_neighbors=8))], verbose=False)

# Máquina de Vectores de Soporte con hiperparámetros calibrados
oraculoDeDelfosMejorado(clasificadorMVSTuneado, datosEntrenamiento, datosPrueba, clasesRealesDEntre, clasesRealesDPrueba)

"""**MÁQUINA DE VECTORES DE SOPORTE:** Apartir de la matrz de la confusión se puede observar que la gran mayoría de los ejemplos se encuentras en la diagonal principal esto es un claro indicativo de que el clasificiador esté, en gran medida, clasificado de manera correcta tanto los verdaderos positivos como los verdaderos negativos, generanfo una precisión muy alta. Posee en general de manera global un 85 % de media para todas las métricas descritas, lo que lo hace un clasificador que quizás sea muy usado en futuros análisis, a pesar de que no sea tan rápido clasificando debido a que cada vez que le llega un ejemplo nuevoa para clasificar se debe recorrer todo el espacio de representación.


"""

# Naive-Bayes Multinomial con hiperparámetros calibrados
oraculoDeDelfosMejorado(clasificadorNBMTuneado, datosEntrenamiento, datosPrueba, clasesRealesDEntre, clasesRealesDPrueba)

"""**NAIVE-BAYES MULTINOMIAL:** Es clasificador concentró en su diagonal principal de la matriz de confusión un número alto de casos, donde los falsos positivos fueron iguales a los falsos negativos, esto a la par de generar un conjunto de métricas con el mismo valor porcentual arrojó como conclusiónes que se trata de un clasificador balanceado en todos sus aspectos. Su precisión y su media armónica(f1-score) son muy altos pero quizás se pueda encontrar mejores alternativas. """

# Vecinos Más Cercanos con hiperparámetros calibrados
oraculoDeDelfosMejorado(clasificadorKNNTuneado, datosEntrenamiento, datosPrueba, clasesRealesDEntre, clasesRealesDPrueba)

"""**VECINOS MÁS CERCANOS:** El más desparejo de todos los clasificadores analizados hasta el momento, es el que tiene valores más ajos en su diagonal principal de la matriz de confusión lo que va a reducir considerablente su precision y su exhaustividad con respecto a los otros clasificadores. Si bien es el mas rá[ido de los tres clasificadores y sus métricas no son bajas personalmete no lo usaría teniendo a la mano otros clasisficadores

Como conclusión a esta parte del analisis el clasificador má recomendado para usar es máquina de vectores de soporte yo que tanto el f1-score como el promedio macro como el promedio pesado nos indican que se mantiene una buena clasisficacion a lo largo de todo el conjunto de prueba lo que nos indica con una seguridad muy alta que ante nuevo ejemplos a tratar la clasificación se mantedrá con los mismo margenes de acierto(calidad por así decirlo)

---

>*  Como último paso de todo este análisis realizaremos lo que llama validación cruzada, con el fin de obtener un número más representativo de las métricas mediadas. Esto se logra tomando en conjunto de datos y particionandoo en N conjuntos disjuntos donde se realiza N pruebas con N-1 conjuntos para entrenamiento y el restante para prueba, se calculan N puntajes y se los promedia, obteniendo un valor más certero para un modelo dado.

Para el análisis de los diferente scores se tomará la media apróximada del mismo para no extender en desía el análisis de las diferentes métricas. Casa análisis se un una método de validación cruzada diferente pero lo que si cabe aclarar es que el proceso se realiza sobre todo el conjunto de datos y son los distintos algoritmos los que se encargan de generar tanto el subconjunto de entrenamiento como el de prueba.
"""

# Se define una función la cual a plartir de un clasificador
# y una serie de métricas a calcular genera diferentes valores
# apartir de un cierto tipo de validación cruzada, mediante la 
# función cross_validate

def generarScores(clasificador, datosEntrenamiento, clasesRealesDEntre, metricas, validacion):
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)
  print(azul,"Se mostraran a continuación los scores de las siguientes métricas solicitadas:",metricas,reset)
  print(azul,"Aplicando la validación cruzada:",validacion,reset)
  tiempoInicio = time.time()
  puntajes=cross_validate(clasificadorKNNTuneado, datosEntrenamiento, clasesRealesDEntre,scoring=metricas,cv=validacion, return_train_score=False,verbose=False, n_jobs=-1)
  tiempoTotal = time.time() - tiempoInicio                                                                       
  print(verde+"Tiempo Total invertido en realizar la validación cruzada:","{0:.4f}".format(tiempoTotal),"segundos"+reset)
  print(rojo, "Scores(mediciones): ")
  pprint(puntajes)
  print(reset)
  print(negro+"--------------------------------------------------------------------------------------------------------------------------------"+reset)

# Defino varios tipos de validacón cruzada
cv_kfold = KFold(n_splits=5)
cv_strat = StratifiedKFold(n_splits=5)
cv_suffle = ShuffleSplit(n_splits=5)
cv_suffle_strat = StratifiedShuffleSplit(n_splits=5)

# Defino un lista de métrica a calcular
metricas = ['precision','recall','f1_macro','f1_weighted']

# Validaciones cruzadas para Máquina de Vectores de soporte
generarScores(clasificadorMVSTuneado, datos.data, datos.target, metricas, cv_kfold)
generarScores(clasificadorMVSTuneado, datos.data, datos.target, metricas, cv_strat)
generarScores(clasificadorMVSTuneado, datos.data, datos.target, metricas, cv_suffle)
generarScores(clasificadorMVSTuneado, datos.data, datos.target, metricas, cv_suffle_strat)

"""> **Máquina de Vectores de Soporte:** 

> > Métricas con mejores valores
* F1-Macro: 0.75881821 obtenida mediante validación aleatoria
* F1-Ponderado: 0.76033765 obtenida mediante validación aleatoria
* Precisión: 0.78333333 obtenida mediante validación estándar
* Recall: 0.75824176 obtenida mediante validación aleatoria

> * **Conclusíon:** si bien al utilizar el método de validación cruzada estándar se obtiene una precisión un poquito más alta que con el resto de validaciones, yo optaría a la hora de validar las métricas la validación cruzada aleatoria, ya que me asegura de que las porciones que se toman en cuenta a la hora de el cálculo de las mediciones son disjuntas y tomadas de manera dispersar o aleatoria sobre al dataset. Esto me lleva tener mayor confianza sobre las métricas obtenidas con esta técnica basando la clasificación en Máquina de Vectores de Soporte.

---
"""

# Validaciones cruzadas para Naive-Bayes Multinomial
generarScores(clasificadorNBMTuneado, datos.data, datos.target, metricas, cv_kfold)
generarScores(clasificadorNBMTuneado, datos.data, datos.target, metricas, cv_strat)
generarScores(clasificadorNBMTuneado, datos.data, datos.target, metricas, cv_suffle)
generarScores(clasificadorNBMTuneado, datos.data, datos.target, metricas, cv_suffle_strat)

"""> **Naive-Bayes Multinomial:** 

> > Métricas con mejores valores
* F1-Macro: 0.73494036 obtenida mediante validación aleatoria estratificada
* F1-Ponderado: 0.73494036 obtenida mediante validación aleatoria estratificada
* Precisión: 0.78333333 obtenida mediante validación estándar
* Recall: 0.72 obtenida mediante validación aleatoria estratificada

> * **Conclusíon:** nuevamente al optar por el método de validación cruzada estándar se obtiene una precisión un poco más elevada que con el resto de validaciones, yo optaría a la hora de validar las métricas, la validación cruzada aleatoria estratificada, ya que me asegura de que las porciones que se toman en cuenta a la hora de el cálculo de las mediciones son disjuntas y tomadas de manera dispersar o aleatoria sobre al dataset y además se las elije de tal forma que en cada partición haya apróximadamente la misma cantidad de representantes de cada clase. Esto genera mucha mayor confianza sobre las métricas obtenidas con esta técnica basando la clasificar con Naive-Bayes Multinomial.

---
"""

# Validaciones cruzadas para Vecinos Más Cercanos
generarScores(clasificadorKNNTuneado, datos.data, datos.target, metricas, cv_kfold)
generarScores(clasificadorKNNTuneado, datos.data, datos.target, metricas, cv_strat)
generarScores(clasificadorKNNTuneado, datos.data, datos.target, metricas, cv_suffle)
generarScores(clasificadorKNNTuneado, datos.data, datos.target, metricas, cv_suffle_strat)

"""> **Vecinos Más Cercanos (KNN):** 

> > Métricas con mejores valores
* F1-Macro: 0.73483427 obtenida mediante validación aleatoria estratificada
* F1-Ponderado: 0.73483427 obtenida mediante validación aleatoria estratificada
* Precisión: 0.78333333 obtenida mediante validación estándar
* Recall: 0.71 obtenida mediante validación aleatoria estratificada

> * **Conclusíon:** Con el método de validación cruzada estándar se obtiene una precisión un poco más elevada que con el resto de validaciones, yo elegiría para validar las métricas, la validación cruzada aleatoria estratificada. Esto genera mucha mayor confianza sobre las métricas obtenidas con esta técnica.

---

## **4.** 
> RandomForest: 
* Conceptualmente en el claficador Ramdom Forest (bosque aleatorio) se cultivan varios árboles(bosque) para clasificar cada nuevo ejemplo(documentos) basado en ciertos atributos. De cada árbol se obtiene una clasificación y finalmente el bosque elige la clasificación con más votos para ese documento. Cuántos más árboles haya en el bosque más robusta y confiable será la clasificación final. 
* Cada árbol es una condición compuesta de varias características, la cual tiene una respuesta positiva o negativa, las hojas del árbol son las respuestas a una clasificación por lo tanto la respuesta que aparesca más veces en las hojas será la clasificación del nuevo ejemplo que se está clasificando.
*Características
* * **I )** Este método de clasificación puede trabajar con grandes cantidades de datos(gran dimensionalidad), incluso a veces usado para reducir la dimensionalidad del conjunto de datos como fase previa de otros métodos de clasificación.
* * **II )** Tiene un método efectivo para estimar datos faltantes y mantiene la precisión cuando falta una gran proporción de los datos.
* * **III )** En ocasiones puede parecer un algoritmo de caja negra ya que se tiene muy poco control sobre lo que realmente está haciendo este modelo.
* Concretamente un Ramdom Forest es un meta estimador que entrena un número determinado de árboles de decisión sobre varios subconjuntos del dataset de entrada y utiliza el promedio para mejorar la precisión y controlar el posible sobre entrenamiento(over-fitting). El tamaño de los subconjuntos es controlado mediante uno de las hiperparámetros que posee, de otra forma se utliza todo el dataset para construir cada árbol de decisión.
* Algunos hiperparámetros del clasificador:
* * n_estimators[int], default=100: Númeor de árboles usados por el clasificador.
* * criterion{“gini”, “entropy”}, default=”gini”: Función que mide la calidad de las partiicones. Mide impuresas además de la entropia para ganancia de información probabilsticamente hablando. Parámetro específico aplicado sobre árboles individuales.
* * max_depth[int], default=None: Máxima profundidad de los árboles. Al usar 'None' los nodos son expandidos hasta ser hoja o hasta que contengan menos de 'min_samples_split' ejemplos.
* * min_samples_split[int ó float], default=2: Número mínimo de ejemplos requeridos para particionar un nodo interno. Si es [float] se toma la función techo('min_samples_split' * 'n_samples')
* * min_samples_leaf[int ó float], default=1: Número mínimo de ejemplos por hoja. Si es [float] se toma la función techo('min_samples_leaf' * 'n_samples')
* * max_features{“auto”, “sqrt”, “log2”}, [int ó float], default=”auto”="sqrt": Número de características a ser consideradas cuando se busca la mejos partición de nodos. Ej: si es [log2], entonces 'max_features' = log2(#CaracterísticasDelDataset).
* * max_leaf_nodes[int], default=None:Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
* * min_impurity_decrease[float], default=0.0: El nodo que se particion debe generar que dichas particiones indiscan una disminución de la impureza mayor o igual a este valor. 
* * class_weight{“balanced”, “balanced_subsample”}, [diccionario], [lista de dicionarios], default=None: Pesos asociados a las clases del dataset en formato {etiqueta_clase: peso}. De no ser especificado se asume que todas las clases tienen peso 1.
* * ccp_alpha[float no negativo], default=0.0: Parámetro de complejidad usado para podas con costo-complejidad minimal. Los subárboles con el costo-complejidad más alto que sea menor que 'ccp_alpha' será elegido para ser podado. Por defencto no se realiza ninguna poda sobre los árboles.

> **Para Más información acerca del clasificador consultar:** 
* *https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html*

## **5. y 6.**

Aclaración importate: unifiqué todo el analisis del clasificador Random Forest (incisos 5 y 6) en un solo, el apartado que inicia aquí, con el objetivo de presentar los datos de manera más clara.

Para llevar a cabo un analisis más acertado aplicaré todos los pasos aplicados para el resto de los clasificadores ya estudiados en este trabajo, con lo cual reutilizaré muchas de las funciones ya implementadas más arriba
"""

# Defino el pipeline para el Random Forest
clasificadorRdmF = Pipeline([ ('VECTORIZADOR', CountVectorizer()), ('TRANSFORMADOR-TFIDF', TfidfTransformer()), ('CLASIFICADOR', RandomForestClassifier())], verbose=True)

# Evalúo el clasificador de manera estándar con el dataset completo
# para ver su compartamiento aproximado
oraculoDeDelfos(clasificadorRdmF, datos)

"""Se puede observar que solo aplicando el clasificador estándar Random Forest sobre el dataset completo las clases predichas tienen una tasa de acierto del 100 %  con respecto a las clases reales. Esto puede inclinar la balanza hacia el lado que indica que se prudujo sobre entrenamiento(over-fitting) a la hora de entrenar el modelo con ese conjunto de datos(para este sao el dataset completo)"""

# Defino la lista de parámetros para la búsqueda exhaustiva con GridSearch
parametrosRdmF = {'VECTORIZADOR__ngram_range':[(1,1),(1,2),(2,2)], 'VECTORIZADOR__stop_words': ['english'], 'VECTORIZADOR__tokenizer':[word_tokenize], 
                  'VECTORIZADOR__preprocessor':[prePrcesamientoPersonalizado], 'TRANSFORMADOR-TFIDF__use_idf': [True,False], 
                  'CLASIFICADOR__criterion': ['gini', 'entropy'], 'CLASIFICADOR__min_samples_split': [2, 5, 10], 'CLASIFICADOR__max_features':['auto', 'sqrt', 'log2'],
                  'CLASIFICADOR__n_jobs': [-1]}

encontrarLoMejor(clasificadorRdmF, parametrosRdmF, datosEntrenamiento, clasesRealesDEntre)

"""> Al usar el clasificador Random Forest, se obtuvo con una taza de acierto de aproximadamente 82.5625 %, aplicando GridSearch, los siguientes valores:

**Pipeline usado**: *{'VECTORIZADOR': CountVectorizer(), 'TRANSFORMADOR-TFIDF': TfidfTransformer(), 'CLASIFICADOR': RandomForestClassifier()}*

**Tiempo Total invertido en encontrar la mejor combinación de hiperparámetros**: 993.3819 segundos

**Con los siguientes valores de parámetros**:  *{'CLASIFICADOR__criterion': 'entropy', 'CLASIFICADOR__max_features': 'sqrt', 'CLASIFICADOR__min_samples_split': 10, 'CLASIFICADOR__n_jobs': -1, 'TRANSFORMADOR-TFIDF__use_idf': False, 'VECTORIZADOR__ngram_range': (1, 1), 'VECTORIZADOR__preprocessor': <function prePrcesamientoPersonalizado at 0x000001F8F30F6C10>, 'VECTORIZADOR__stop_words': 'english', 'VECTORIZADOR__tokenizer': <function word_tokenize at 0x000001F8EBD1D820>}*
"""

# Defino los clasificadores con los parámetros ya establecidos
clasificadorRmdFTuneado = Pipeline([ ('VECTORIZADOR', CountVectorizer(ngram_range=(1,1), stop_words='english', tokenizer=word_tokenize, 
                                    preprocessor= prePrcesamientoPersonalizado)), ('TRANSFORMADOR-TFIDF', TfidfTransformer(use_idf=False)), 
                                    ('CLASIFICADOR', RandomForestClassifier(criterion='entropy', max_features='sqrt', min_samples_split=10, n_jobs=-1)) ], verbose=True )

# Invoco la siguiente fución me calcula una serie de métricas con el fin de compararlas con el resto de clasificadores
oraculoDeDelfosMejorado(clasificadorRmdFTuneado, datosEntrenamiento, datosPrueba, clasesRealesDEntre, clasesRealesDPrueba)

"""Habiendo aplicado el pipeline definido con el clasificador Random Forest con la mejor combinación de hiperparámetros en general la fase de entrenamiento insume menos cantidad de tiempo que el resto de clasificadores.

Si bien el porentaje de exito obtenido es un poco menor que la obtenida con Máquina de Vectores de Soporte el hecho de de haber consumido menos tiempo en entrena el modelo lo hace un mejor opción.

De la matriz de confusión se puede observa que el número total que conforma la diagonal principal el mayor a la obtenida en el resto de clasificadores en este trabajo analizados con lo cual para el dataset suministrado el clasificador Radom Forest realiza una clasificación más precisa

Con lo que respecta a la precisión y al recall si puede sacar en claro que retorna unos valores muy altos, 82% y 89% para reseñas negativas y 88% y 80% para reseñas positivas estos recusltados hacen al clasificador solo comparable con el propuesto con Máquinas de Vectores de Soporte, pero a mi parecer Random Forest se hacerca más al óptimo ya que en repetidas pruebas que fui haciendo revisando estas métricas casi en todas las oportunidades su tiempo de ejecución es menor al resto. Cabe recalcar que estas mediciones son muy dependientes del conjunto de datos inicial. 
"""

# Validaciones cruzadas para Random Forest
generarScores(clasificadorRmdFTuneado, datos.data, datos.target, metricas, cv_kfold)
generarScores(clasificadorRmdFTuneado, datos.data, datos.target, metricas, cv_strat)
generarScores(clasificadorRmdFTuneado, datos.data, datos.target, metricas, cv_suffle)
generarScores(clasificadorRmdFTuneado, datos.data, datos.target, metricas, cv_suffle_strat)

"""> **Random Forest:** 

> > Métricas con mejores valores
* F1-Macro: 0.72932331 obtenida mediante validación aleatoria estratificada
* F1-Ponderado: 0.72932331 obtenida mediante validación aleatoria estratificada
* Precisión: 0.78333333 obtenida mediante validación estándar
* Recall: 0.68932039 obtenida mediante validación aleatoria

> * **Conclusíon:** Los valores más altos de precisión y recall correspondes a las validaciones estándar y alatoria respectivamente. Por otro lado los valores más altos de F1-Macro y F1-Weighted(ponderada) se obtuvieron mediante validacion cruzada aleatoria estratificada. Esto nos hace inclinar por este ultimo metodo de validacion cuando de Radom forest se trata a la hora de validar las métricas. Que estas dos métricas tengan un valor alto, de la mano con que la definición intrínseca de las mismas conlleve el recorrido de todos los ejemplos del dataset para calcularse y bajo los efectos de la configuración de los hiperparámetros que surgieron de aplicar GridSearch con el dataset provisto por la cátedra, tienden fuertemente a que el método de clasificacón Radom Forest sea elegido en futuras ocasiones para continuar, en un escenario hipotético, con la clasificación de nuevos ejemplos.

## Nota final: Todas los valores en este trabajo obtenidos cambirán al volver a ejecutar todas las celdas de código, con lo cuál es menester aclarar que todos los analisis aquí efectudos se realizaron con ejecuciones arbitrarias a la hora de confeccionar dichos analisis.
---
"""